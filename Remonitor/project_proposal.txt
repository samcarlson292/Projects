Project Name: Remonitor: A Resource Monitor for Remote Systems

Team Members: Sam Carlson, Mitchell Masia

Project Abstract:
	We plan to build an applicaton that monitors and tracks system information from multiple remote machines concurrently.  The incoming system information from each remote machine will be aggregated on one master machine and visualized at some interval.  This system will provide a graphical view of CPU utilitzation, virtual memory utilization, swap memory utilization, and disk utilization that can be used by system administrators and scrum masters to reliably track the state of mission-critical distributed machines that so many organizations rely on.  There are several existing implemetations of the Remonitor system; however, the implementation offered here will focus on simplicity and reliability, rather than providing extensive GUI tools.  There is a clear need for a system like Remonitor that is simple and reliable especially in the growing technology market where the Internet-of-Things (IoT) is creating incredibly complex distributed networks that must be watched for anomalies and failures to prevent disasters and loss of time and money.

System Architecture:
	The Remonitor system will work using a relatively simple system architecture in order to meet the goal of total system simplicity.  In prototyping the Remonitor system, we solely focus on monitoring several systems on a single local network in order to initially avoid problems with network communication between remote networks with NATs and Firewalls.  Broadly, each remote machine (not the master) is simply responsible for collecting system information about its performance, packaging that information into an easily transmittable message format, and sending this packaged message.  Each of these machines will run a "server" Python process that runs until signaled (failure or manually by the system admin.).  This process will be multi-threaded and will take care of system information collection, message packing, and message publishing.  The other half of the Remonitor system takes place on the local (master/aggregator) machine.  The master machine is responsible for receiving (and handling) the incoming packaged messages from the remote machines, unpacking these incoming messages, aggregating the unpacked data, and visualizing the data.  This process will not be multithreaded.  The approach to building Remonitor is less object-oriented and more procedural in scheduling and synchronizing multiple threads of execution across distributed machines.

The Remote ("Server") Process:
	Each remote machine being monitored will run its own instance of a Pythonscript.  This process will be multi-threaded and will handle the following tasks: system information acquisition (A), information packaging (B), and package publication (C).  Because these tasks must be executed sequentially for each sample of system information, we use several synchronization techniques in order to assure correct execution of the program and avoid race conditions.  First, we create a data structure (information object) to hold the system information we collect.  This information object will contain System CPU utilization(% of capacity), Virtual Memory Utilization(used, total and %), Swapspace Usage(used, total, and %), Disk Usage(used, total and %), and CPU utiliazation and number threads for the top 5 most active processes.  Whenever we access or mutate the state of this object, we protect it with its own dedicated mutex (mutual exclusion binary semaphore) in order to assure only one thread is working with the object at a given time.  Threads will make use of the "psutil" Python package in order to acquire the aforementioned system information without explicitly making system calls in Python. This has the added advantage of making the server multi-platform compatible.  When the infostore object is full, we use the msgpack utility to pack the Python dictionary object into a bytestream.  Then, a ZeroMQ PUSH socket sends the message via a localhost connection on a specific port to the aggregator/visualizer

The Aggregator ("Master") Process:
	On the single, local, aggregator machine, we run a different Python process.  This process will be slightly less complicated than the server process as it is solely responsible for receiving the incoming messages from multiple servers, separating them on a per-client basis, and visualizing the appropriate information.  To separate the messages by sender, we use a "message filter" which is unique for each sender.  From there, the designated information from each client is kept in its own array, and graphed together on the same plots using Matplotlib.

 Usage of MessagePack:
	MessagePack is used to pack the filled data structure of our infostore class, a Python dictionary, into a bytestream which is easily sent over the network as a ZeroMQ message.  On the client side, MessagePack unpacks the bytestream back into the dictionary that is filled with system info.  The client pulls information out of this dictionary, stores it in local data structures and uses those to graphically display the system info.

Usage of psutil:
	Data is grabbed using psutil, a wrapper around basic system calls for performance information, and is then aggregated into an infoStore object which is packaged, sent, received, unpacked, and plotted by the Remonitor system.

Usage of ZeroMQ:
	ZeroMQ is used to implement our network.  We chose to use the PUSH-PULL model in order to ensure fair-queueing of updates from each remote macine.  The PUSH-PULL model also supports our many-to-one relationship between servers and clients.  Each server PUSH socket is connected to a unique localhost port, passed as a command line argument.  The client PULL socket is connected to each of potentially many ports also passed as command line arguments.  The PULL receives messages in a loop, which come in from the set of servers in a fair-queued manner to ensure even amounts of updates from each remote machine.

Usage of Matplotlib:
	For simplicity we only graph the CPU utilization, virtual memory utilization, swap memory utilization, and disk utilization per remote machine.  While we collect information on process information, we have decided to leave plotting this specific info out of the scope of this project.

Testing Details:
	We will use test-driven development to hit project milestones and make clear progress while building the Remonitor system.  To begin, we isolate the different components of the system to be psutil, MessagePack, ZeroMQ, and Matplotlib.  Out of these components, we have only worked with Matplotlib prior to this project.  To begin, we want to build test systems with each component separately to get a feel for each and how to use them.  Beginning with psutil, we try to retrieve all of the necessary system info we will be plotting and monitoring (the same that a system admin would like to keep tabs on).  Once we can acquire that information correctly, the next step is to use MessagePack in conjunction with ZeroMQ in order to build a system to send simple messages back and forth between clients with hardcoded, known IP addresses.  For now this is just sending messages to the localhost on our laptops, but will soon be extended to send information between physical clients.  Next, we want to be able to decompose the information gathered by psutil into messages which we can send from client to client.  This will be done by sending the information as a simple byte stream (created using MessagePack) such that we are not bound to using a specific message format (JSON, Python Object) in the future for encoding, sending, receiving, and decoding the messages. Last, in the aggregator process, we plan to use the message received from the server with the utilization information and plot that at some interval.  The figure we image plotting includes four graphs, one for CPU utilization, virtual memory utilization, swap memory utilization, and disk utilization.  Our testing for this component will be focused on learning how to plot continuously while updating the data structures we plan to visualize.  Once we have tested each component individually, we plan to integrate these base systems into modified processes (server and master) that will be capable of using the four individual components to monitor remote systems.

Difficulties Expected:
	We expect major difficulties not from MessagePack or psutil; however, we expect that using ZeroMQ and Matplotlib (for live-plotting application) will be difficult.

Success:
	A success in this project for us is defined as learning to use the three foreign components to build this remote system monitor.  Additionally, in the end, creating a useful application that a system administrator could use to monitor anomalies in server performance would be very fulfilling, and definitely a huge success for us.
